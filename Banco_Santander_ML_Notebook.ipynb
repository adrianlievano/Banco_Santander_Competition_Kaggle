{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Background Information ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Financial institutions, like Santander, help people and businesses prosper by providing tools and services to assess their personal financial health and to identify additional ways to help customers reach their monetary goals. In the United States, it is estimated that 40% of Americans cannot cover a $400 emergency expense1. As a result, it is imperative that financial institutions learn consumer habits to adopt new technologies to better serve their financial needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Santander, a financial institution, is trying to predict the next transaction a given customer is trying to complete based on historical banking information. This is a binary classification problem where the input data contains 299 unnamed normally-distributed feature variables. The solution to this problem will be evaluated on a provided test data set by Santander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Statement #\n",
    "\n",
    "The provided train.csv file contains 200,000 unique rows corresponding to customer data. Given the large dataset, and the need to complete binary classification, there are many solutions to this problem: mine will involve using a deep neural network, after preprocessing the inputs by normalizing and scaling features, to classify the two target variables. After the model is trained and validated on a subset of the data from the train.csv file, I will run my trained model on the provided test set from Santander and measure the accuracy of each prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1ec16130193c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m \u001b[1;31m# linear algebra\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# data processing, CSV file I/O (e.g. pd.read_csv)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the Santander dataset\n",
    "train_data = pd.read_csv('../input/train.csv')\n",
    "test_data = pd.read_csv('../input/test.csv')\n",
    "submission_data = pd.read_csv('../input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration & Visualizations ## \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of training data\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['target'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Immediate Key Takeaways: The mean, standard deviation, and maximum values of the features vary widely; if we choose to implement a black-box algorithm, like neural networks, a key step will be data preprocessing, which will involve feature scaling, potentially outlier-detection and removal, and definitely normalization of these variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Separate Training and Validation Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_data = train_data.drop(['ID_code','target'], axis=1) #remove target column & ID column\n",
    "\n",
    "#Split the dataset into training and validation sets so that we can implement models without touching \n",
    "#the test set\n",
    "X_train, X_val, y_train, y_val = train_test_split(feature_train_data, train_data['target'], test_size = 0.20, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the dataset statistics, we can see that the mean and standard deviatation of each var feature significantly varies. If we apply supervised learning algorithms without any feature scaling or preprocessing, the algorithm will bias to certain features over others when learning the relationship between the input features and the target classification of the customer.\n",
    "\n",
    "Binary classification without any preprocessing using a simple decision tree classifier could provide insight into what minimum baseline performance we achieve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "treeClassifier = DecisionTreeClassifier()\n",
    "treeClassifier.fit(X_train, y_train)\n",
    "\n",
    "y_pred = treeClassifier.predict(X_val)\n",
    "\n",
    "# TODO: Report the score of the prediction using the testing set\n",
    "score = accuracy_score(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare input and output; use decision tree classifier to make predictions \n",
    "X_test = test_data.drop('ID_code', axis=1)\n",
    "y_test_ID = test_data['ID_code']\n",
    "y_pred_testSet = treeClassifier.predict(X_test)\n",
    "\n",
    "#Convert array into panda dataframe\n",
    "F=pd.DataFrame(np.vstack(y_pred_testSet) , columns = ['target'])\n",
    "\n",
    "#Build output submission dataframe\n",
    "competitionSub = pd.concat([y_test_ID, F], axis=1)\n",
    "competitionSub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Model Performance - Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the test set, when we submit to Kaggle, a decision tree classifier gets us 55% accuracy, which is slightly better than random guessing. Alas, it's not a great solution to this problem and now we'll dive deeper into data exploration. (I placed 7,000+, YIKES!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we saw redundant features in the dataset, we might see strong linear correlatations to multiple features; this also lets us see whether or not our features are normally distributed or if the dataset contains any outliers that could skew our results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_data.corr()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming & Normalizing Numerical Features\n",
    "If our data is not normally distributed, especially if the mean and medians vary significantly (indicating a large skew in the data), it's appropriate to apply a non-linear scaling to reduce this; one example is applying the natural logarithm to the dataset.\n",
    "\n",
    "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as 'capital-gain' or 'capital-loss' above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning. Care must be taken when applying this transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_features_train = {} #store scaling values for conversion back to original values later on\n",
    "scaled_features_test = {} #store scaling values for conversion back to original values later on\n",
    "\n",
    "feature_test_data = test_data.drop(['ID_code'], axis=1)\n",
    "\n",
    "#Continuous Variables are set to be between 0 & 1 and to have zero mean and a standard deviation of 1# Yielded 91% accuracy with 50 epochs\n",
    "for each in cont_columns:\n",
    "    mean_train, std_train =feature_train_data[each].mean(), feature_train_data[each].std()\n",
    "    scaled_features_train[each]= [mean_train , std_train]\n",
    "    feature_train_data.loc[:,each] = (feature_train_data[each] - mean_train)/std_train\n",
    "    \n",
    "    mean_test, std_test = feature_test_data[each].mean(), feature_test_data[each].std()\n",
    "    scaled_features_test[each]= [mean_test , std_test]\n",
    "    feature_test_data.loc[:,each] = (feature_test_data[each] - mean_test)/std_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_train_data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaway: Look closely at the standard deviation, mean, min, and max of some of the first few var features. We standardized it and made the mean approach the value of 0, meaning that a supervised learning algorithm can now take these inputs and output a more equal understanding of the blend of features to identify the two targets.\n",
    "\n",
    "From another posted kernel on Kaggle, we observed that this was an unbalanced classification challenge (Link: https://www.kaggle.com/allunia/santander-customer-transaction-eda)\n",
    "\n",
    "So, I used their visualization as inspiration to provide readers of this kernel a figure to view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(train_data.target.values, palette=\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Dimensionality Reduction (Principal Component Analysis)Â¶\n",
    "It's worth understand how much of the input data is redundant, or less important when classifying the two classes. In our case, we'll implement Principal Component Analysis (PCA), a form of unsupervised learning, that will define how much of the dataset's variance is explained by a number of features we choose to explore. We'll work with the original data and NOT the data that was normalized with zero mean and standard deviation of 1. That makes it harder to figure out which feature is most responsible for the variance in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#Separate preprocessed training and validation sets\n",
    "X_train_prep, X_val_prep, y_train, y_val = train_test_split(feature_train_data, train_data['target'], test_size = 0.20, random_state = 25)\n",
    "\n",
    "# Compute a PCA (eigenfaces) on the face dataset (treated as unlabeled\n",
    "# dataset): unsupervised feature extraction / dimensionality reduction\n",
    "n_components = 5\n",
    "\n",
    "#Extracting the top input variables from 200,000 samples of training data after standard scaling\n",
    "#was applied; capture 95% of the variance in the number of components we are reducing down to.\n",
    "pca = PCA(.95,\n",
    "          whiten=True).fit(X_train_prep)\n",
    "\n",
    "#Projecting the input data onto the most important principal axises\n",
    "X_train_pca = pca.transform(X_train_prep)\n",
    "X_val_pca = pca.transform(X_val_prep)\n",
    "pca.explained_variance_ratio_.cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaway: We can confirm that the cumulative sum of the variance explained by the features is 0.9531571, a number we set when we implemented PCA above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pca.n_components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "key takeaway: it looks like that with 190 of the 200 features, we can capture 95% of the variance of the data. We reduced the dimensions, but it's worthwhile to note that this is not a significant reduction in dimensionality of the training data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing PCA on Original Dataset For Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting the top input variables from 200,000 samples of training data after standard scaling\n",
    "#was applied; capture 95% of the variance in the number of components we are reducing down to.\n",
    "pca1 = PCA(.95,\n",
    "          whiten=True).fit(X_train)\n",
    "\n",
    "#Projecting the input data onto the most important principal axises\n",
    "X_train_pca1 = pca1.transform(X_train)\n",
    "X_val_pca1 = pca1.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pca1.n_components_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaway: PCA on the original training set, without any standard scaling (i.e., setting the mean to 0 and standard deviation to 1 for each feature), yields fewer principal components that capture 95% of the variance in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Use PCA values from the preprocessed input data after mean is set to zero and standard devitation is set to 1.\n",
    "logisticRegr0 = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr0.fit(X_train_pca, y_train)\n",
    "y_pred_logisticRegr0 = logisticRegr0.predict(X_val_pca)\n",
    "\n",
    "logisticRegr1 = LogisticRegression(solver = 'lbfgs')\n",
    "logisticRegr1.fit(X_train_pca1, y_train)\n",
    "y_pred_logisticRegr1 = logisticRegr1.predict(X_val_pca1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0ac739964158>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#This scores our logistic regression using the scaled data and after applying PCA; here we use 190 feature variables that capture 95% variance in the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mscore0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred_logisticRegr0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mscore0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "#This scores our logistic regression using the scaled data and after applying PCA; here we use 190 feature variables that capture 95% variance in the data\n",
    "score0 = accuracy_score(y_val, y_pred_logisticRegr0)\n",
    "score0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scores our logistic regression using the raw data and after applying PCA; here we use 111 feature variables that capture 95% variance in the data\n",
    "\n",
    "score1 = accuracy_score(y_val, y_pred_logisticRegr1)\n",
    "score1\n",
    "\n",
    "#On the submission page, this yielded a test set score of ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaway: It seems that a simple logistic regression performs better in the scenario where we set the mean to zero and the standard deviatation to 1 of the raw data provided. Then, we apply PCA and then we fit a logistic regression to the transformed feature data that captures 95% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement ADABoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bdt = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=1)\n",
    "\n",
    "bdt.fit(X_train_pca, y_train)\n",
    "y_pred_bdt = bdt.predict(X_val_pca)\n",
    "score_bdt = accuracy_score(y_val, y_pred_bdt)\n",
    "score_bdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaway: Here, we're using ensemble methods with AdaBoost to see if we can get a better results on the test set; we notice that score_bdt > score_0, which represented the logistic regression. Our submission result slightly increased to 0.637. Time to bring in the big guns... Neural Networks.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Neural Network ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5836808ac1f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Import Necessary Libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSGD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import Necessary Libraries\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "#To get categorical output, we're going to one-hot encode the output vector for our train and validation dataset.\n",
    "targets = np.array(keras.utils.to_categorical(y_train, 2))\n",
    "targets_val = np.array(keras.utils.to_categorical(y_val, 2))\n",
    "\n",
    "# Building the model\n",
    "model = Sequential() #\n",
    "model.add(Dense(1024, activation='relu', input_shape=(X_train_prep.shape[1],)))\n",
    "#model.add(Dropout(rate=1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(rate=1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(254, activation='relu'))\n",
    "#model.add(Dropout(rate=1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(254, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dropout(rate=.1))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes for Building a Neural Network Architecture 1.) Beware overfitting: too many neurons, too many layers, or too little data will overfit your training set and result in worse performance on test set data.\n",
    "\n",
    "2.) Batch normalization between relu activation functions is a good way to ensure samples are less skewed while entering subsequent layers. It is however, more computationally expensive to complete.\n",
    "\n",
    "3.) Beware setting too many epochs to train. It leads to overfitting at times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Neural Network ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-640261e7e399>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;31m# features and targets are Numpy arrays --just like in the Scikit-Learn API.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Set hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "# features and targets are Numpy arrays --just like in the Scikit-Learn API.\n",
    "\n",
    "#Set hyperparameters \n",
    "epochs = 15\n",
    "batch_size = 300\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(X_train_prep, targets,\n",
    "          epochs=epochs, batch_size=batch_size,validation_data = (X_val_prep, targets_val), callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred_nn = model.predict(X_val_pca)\n",
    "#score_nn = accuracy_score(y_val, y_pred_nn)\n",
    "#score_nn\n",
    "best_weights_filepath = \"weights.best.from_scratch.hdf5\"\n",
    "model.load_weights(best_weights_filepath)\n",
    "\n",
    "score = model.evaluate(X_val_prep, targets_val)\n",
    "print(\"\\n Training Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Set Neural Network Performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare input and output; \n",
    "#Prepare input and output; use decision tree classifier to make predictions \n",
    "#X_test = pca.transform(feature_test_data)\n",
    "X_test = feature_test_data\n",
    "\n",
    "y_test = test_data['ID_code']\n",
    "y_pred_testSet = model.predict(X_test)\n",
    "y_pred_testSet=pd.DataFrame(y_pred_testSet)\n",
    "y_pred_testSet = pd.concat([y_test, y_pred_testSet],axis=1)\n",
    "y_pred_testSet['target'] = np.where(y_pred_testSet[0] > y_pred_testSet[1], 0, 1)\n",
    "drop = [0,1]\n",
    "submission=y_pred_testSet.drop(columns=drop, axis =1)\n",
    "\n",
    "submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key takeaways: The neural nets didn't do so hot! Submission score was .639. Now, we should probably try something like XGboost with GridSearchCV optimization to see if we can identify the most important features contributing to the targets and go from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Adrian_Lievano_NN', columns = ['ID_code','target'], index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement XGBoost Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(objective = 'binary:logistic', random_state = 42)\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": [0.03, .1, 0.3], # default 0.1 \n",
    "    \"max_depth\": [2, 6], # default 3\n",
    "    \"n_estimators\": [10, 20, 100], # default 100\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(xgb_model, param_distributions=params, random_state=42, n_iter=200, cv=3, verbose=1, n_jobs=1, return_train_score=True)\n",
    "\n",
    "search.fit(X_train_prep, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = search.predict(X_val_prep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_xgb = accuracy_score(y_val, y_pred_xgb)\n",
    "score_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = feature_test_data\n",
    "\n",
    "y_test = test_data['ID_code']\n",
    "y_pred_testSet = search.predict(X_test)\n",
    "y_pred_testSet=pd.DataFrame(np.vstack(y_pred_testSet) , columns = ['target'])\n",
    "competitionSub_xgb = pd.concat([y_test, y_pred_testSet], axis=1)\n",
    "competitionSub_xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "competitionSub_xgb.to_csv('AAL_xgb.csv', columns = ['ID_code','target'], index = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions:\n",
    "\n",
    "Supervised machine learning algorithms, including neural networks, are not enough to get the highest accuracy on the test set; instead, it's just as important to understand and visualize the statistics in your provided dataset.\n",
    "\n",
    "This notebook walks Kagglers through a variety of supervised machine learning algorithms, presenting the tradeoffs and performance numbers on the test set for this data competition.\n",
    "\n",
    "I still have much to learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
